{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beysian Regression\n",
    "\n",
    "Consider the linear regression model specification, where we assume the real-world linkage between y and x (and of course $\\epsilon$, but this is generally ignored because it is unobservable) is given by,\n",
    "\n",
    "$$y_i =\\alpha+\\beta x_i + \\epsilon$$\n",
    "\n",
    "If we run OLS, we can get an estimator without any further thoughts. We just minimize the sum of squired error, solve the first-order condition and \n",
    "\n",
    "$$(\\hat{\\alpha}, \\hat{\\beta}) \\equiv arg \\min_{\\alpha,\\beta} [y_i-(\\alpha+\\beta x_i)]^2$$\n",
    "\n",
    "Done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequentist way of analysis\n",
    "A little more chaos is introduced when we want to analyze the *properties* of the estimators $\\hat{\\alpha}, \\hat{\\beta}$. More specifically, we assume that x_i is given, but $\\epsilon$ is a random variable (because this is the real-life situation), then we ask the question \"*What is the distribution of $\\hat{\\beta}$ ?*\"\n",
    "\n",
    "Given the question, we recall the fact that $\\hat{\\alpha}, \\hat{\\beta}$ is a function of y\n",
    "\n",
    "$$\\hat{\\beta} = \\frac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum (x_i-\\bar{x})^2}$$ \n",
    "\n",
    "or in a more general form \n",
    "\n",
    "$$\\hat{\\beta} = (X'X)^{-1}X'y$$\n",
    "\n",
    "So that the conditional distribution of $\\hat{\\beta}$ should be given by\n",
    "\n",
    "$$\\hat{\\beta}|X \\sim \\mathcal{F}[(X'X)^{-1}X'y|X]$$\n",
    "\n",
    "While the simplest distribution is a normal distribution, we naturally assume $\\epsilon|X \\sim \\mathcal{N}(0,\\sigma^2)$. And by property of a normal distribution, we have\n",
    "\n",
    "$$y|X \\sim \\mathcal{N}(X\\beta,\\sigma^2)$$\n",
    "$$\\hat{\\beta}|X \\sim \\mathcal{N}(\\beta,(X'X)^{-1}\\sigma^2)$$\n",
    "\n",
    "Now since we have the distribution of $\\hat{\\beta}$, you can calculate $E[\\hat{\\beta}|X]$, $Var[\\hat{\\beta}|X]$, or do any probablistic analysis you like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian way of analysis\n",
    "\n",
    "Great minds sometimes don't think alike!\n",
    "\n",
    "Bayesian statistics argue that given the final goal of deriving a distribution, why don't we target at it in the first place? We can then use the distribution mean to derive an estimator and call it $\\hat{\\beta}_{Bayes}$.\n",
    "\n",
    "\n",
    ">What motivates Bayesian staticists to derive a distribution first? Recall the Beyesian rule,\n",
    "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{P(B|A)*P(A)}{P(B)}$$\n",
    "Isn't this just a stacking of formula? Partially Yes. But it is no longer merely a rearrangement when it is granded a Beyesian interpretation. To see this, let's interpret and define <br/>\n",
    "$P(A)$ as the *prior* - think of this as your initial belief of the distribution of A (Today is going to be sunny!).<br/>\n",
    "the quotient $\\frac{P(B|A)}{P(B)}$ represents the *New Information* - clouds crowd the sky and you hear the thunder. <br/>\n",
    "$P(A|B)$ as the *posterior* - this is the updated belief after incorporating news that B is true. - Oh no! I was wrong, it is going to rain. <br/>\n",
    "!!! Intuitive and useful, isn't it? That's why Bayes rule turns out to be so popular. Once you know the probability distribution of the new information $\\frac{P(B|A)}{P(B)}$, you can update your *a priori* belief to an *a posteriori* DISTRIBUTION.\n",
    "\n",
    "\n",
    "Now that Bayesian rule deals with distribution by default, let's try to apply it to this situation. The question can be tranlated to:\n",
    "\n",
    ">We have *a priori* belief of the distribution of all model prameters $\\alpha, \\beta, \\sigma$. <br/>\n",
    "The new information is the dataset ${(y_i,x_i)}_{i=1}^N$ and the assumption that y is normally distributed, from which we need to calculate $\\frac{P(B|A)}{P(B)}$<br/>\n",
    "Then we ask what is the *a posteriori* distribution of $\\alpha, \\beta, \\sigma$\n",
    "\n",
    "The Bayesian set up is more flexible in that we can integrate our *a priori* belief or guess of the parameter distributions. In contrast, there is no such step in a OLS regression.\n",
    "\n",
    "The steps to find the answer is never so clear - use the Bayesian Rule\n",
    "\n",
    "$$f(\\alpha, \\beta, \\sigma|y=\\{y_i\\}, X=\\{X_i\\}) = \\frac{f(y| X,\\alpha, \\beta, \\sigma)* f(\\alpha, \\beta, \\sigma)*f(X)}{f(y|X)*f(X)}|_{y=\\{y_i\\}, X=\\{X_i\\}} \\propto f(y|X,\\alpha, \\beta, \\sigma)* f(\\alpha, \\beta, \\sigma)|_{y=\\{y_i\\}, X=\\{X_i\\}}$$\n",
    "\n",
    "> $x \\propto f(x)$ means $x = kf(x)$. Such simplification is rational in probability theory, because (1) we can figure out k later using the fact that $\\int f(x) dx = 1$. (2) k is usually known as the normalization part, and won't affect the distribution. By ignoring k i.e. any part that is multiplicative and that does not include key variable(s) $x$, we are able to focus on the core structure of the distribution. \n",
    "\n",
    "Everything is this equation is either *a priori* assumed or known from data. You can start from here to calculate the marginal distribution of any parameter. \n",
    "\n",
    "Problem solved! This is the distribution you want. There you go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------\n",
    "Wait, one more thing! What if we need a point estimator, the $\\hat{\\beta}_{Bayes}$?\n",
    "\n",
    "Come on, you already know the whole picture - the distribution! Just pick any value with a good statistical interpretation.\n",
    "\n",
    "You can calculate the conditional expectation $\\hat{\\beta}_{Bayes} = E[\\beta|y,X,\\sigma] = \\int \\beta * f(\\beta|y,X,\\sigma) d\\beta$, then plug in the values of y, X, and sometimes $\\sigma$\n",
    "\n",
    "Or you could use maximum a posteriori estimation,\n",
    "\n",
    "$$(\\hat{\\alpha},\\hat{\\beta},\\hat{\\sigma}) \\equiv arg \\max_{\\alpha,\\beta,\\sigma} f(y|X,\\alpha, \\beta, \\sigma)* f(\\alpha, \\beta, \\sigma)|_{y=\\{y_i\\}, X=\\{X_i\\}}$$\n",
    "\n",
    "The Bayesian regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to link Beyasian regression to OLS\n",
    "\n",
    "For simplicity, let's add $\\alpha$ to the vector $\\beta$. Assume the prior to be as follows:\n",
    "\n",
    "$P(\\beta) \\propto 1$, and $P(\\sigma^2) \\propto \\frac{1}{\\sigma^2}$\n",
    "\n",
    "Then \n",
    "\n",
    "$$\\beta|\\sigma, y, X \\sim \\mathcal{N}((X'X)^{-1}X'y,(X'X)^{-1}\\sigma^2)$$\n",
    "\n",
    "$$\\hat{\\beta}_{Bayes} = E[\\beta|\\sigma, y, X] =(X'X)^{-1}X'y$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
