{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a20ee0ab-1023-4921-bfca-7f1948cfd2ec",
   "metadata": {},
   "source": [
    "# 8 OLS Matrix Form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bde657-facf-41fe-8a2b-6c9585d45b39",
   "metadata": {},
   "source": [
    "In a given dataset, the linear relationship between dependent and independent variables can be expressed in a general format as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    y_1 &= \\beta_0 + \\beta_1 x_{11} + \\beta_2 x_{12} + ... + \\beta_k x_{1k} + u_1 \\\\\n",
    "    y_2 &= \\beta_0 + \\beta_1 x_{21} + \\beta_2 x_{22} + ... + \\beta_k x_{2k} + u_2 \\\\\n",
    "    &... \\\\\n",
    "    y_n &= \\beta_0 + \\beta_1 x_{n1} + \\beta_2 x_{n2} + ... + \\beta_k x_{nk} + u_n \\\\ \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e25941c-30b4-475c-807b-62c043c8a326",
   "metadata": {},
   "source": [
    "which is essentially a linear equation system, and therefore linear algebra comes into play. For simplicity, this linear equation system can be expressed in a matrix form,\n",
    "\n",
    "$$\\vec{y} = X\\vec{\\beta} + \\vec{u}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801a1864-d428-4e01-835e-ee9f123ef895",
   "metadata": {},
   "source": [
    "where $\\vec{y} = (y_1,....,y_n)^T$, $\\vec{\\beta}=(\\beta_0,...,\\beta_k)^T$, $\\vec{u} = (u_1,...,u_n)^T$, and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea8e93a-6da9-4d50-bc18-38c758721480",
   "metadata": {},
   "source": [
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "    1 & x_{11} & x_{12} &\\dots & x_{1k} \\\\\n",
    "    1 & x_{21} & x_{22} &\\dots & x_{2k} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    1 & x_{n1} & x_{n2} & \\dots & x_{nk}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5903dc8-5e00-4376-9d55-9ddb27ae9082",
   "metadata": {},
   "source": [
    "X is also known as the **design matrix**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a0e9f1-a37a-4fa0-ae87-46a5765753be",
   "metadata": {},
   "source": [
    "# 8.1 Why matrix form?\n",
    "\n",
    "Matrix form simplifies the presentation of a linear system, and enables us to deal with multiple equations at a time. There are profound theorems and properties in **linear algebra** that can help us draw a conclusion within few steps.\n",
    "> If you are interested in matrix forms, you can refer to textbooks for linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d041218f-0200-499f-8234-dcbf2a911ab3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 8.2 Matrix addition and scalar multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e97669-b14b-4c50-bdc9-a49cdd39d87d",
   "metadata": {},
   "source": [
    "Adding matrices is very simple. Just add each element in the first matrix to the corresponding element in the second matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db6cbe5-92cc-4f52-860d-391d7c89f6cc",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{bmatrix}\n",
    "    1 & 2 \\\\\n",
    "    3 & 4\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "    2 & 3 \\\\\n",
    "    4 & 5\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    1+2 & 2+3 \\\\\n",
    "    3+4 & 4+5\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    3 & 5 \\\\\n",
    "    7 & 9\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b3a3f-dcd1-4893-8d8e-1fccbfb607d1",
   "metadata": {},
   "source": [
    "In order to add two matrices, the entries must correspond. Therefore, addition and subtraction of matrices is only possible when the matrices have the same dimensions.  Matrix addition is commutative and is also associative, so the following is true:\n",
    "\n",
    "$A+B = B+A$ \\\n",
    "$(A+B)+C = A+(B+C)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290d9605-a026-4994-8025-30126c15c8ce",
   "metadata": {},
   "source": [
    "Scalar multiplication of a matrix is done by multiplying each element by that scalar.\n",
    "\n",
    "$\n",
    "3 \\times \\begin{bmatrix}\n",
    "    1 & 2 \\\\\n",
    "    3 & 4\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    3 \\times 1 & 3 \\times 2 \\\\\n",
    "    3 \\times 3 & 3 \\times 4\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    3 & 6 \\\\\n",
    "    9 & 12\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddae366-21dc-4efc-a801-ed8b99fd2c26",
   "metadata": {},
   "source": [
    "And we have the following properties for scalar multiplication of a matrix\n",
    "\n",
    " - Left and right distributivity: $c(A+B) = cA+cB = (A+B)c$\n",
    " - Associativity: $cdA = c(dA)$\n",
    " - Null: $0\\times A = 0_{mxn}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa7570-cf7b-4afa-99b4-133cc288c006",
   "metadata": {},
   "source": [
    "The \"Identity Matrix\" is the matrix equivalent of the number \"1\":\n",
    "\n",
    "$I_{3x3} = \\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52149ceb-56ad-429e-abe0-53f0af7347f7",
   "metadata": {},
   "source": [
    "# 8.3 Transpose of a Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c27b842-ecaf-4c78-b622-d13809d2ca0f",
   "metadata": {},
   "source": [
    "The Transpose of a matrix means flipping a matrix over its diagonal; \n",
    "\n",
    "If $A = \\begin{bmatrix}\n",
    "    1 & 2 \\\\\n",
    "    3 & 4\n",
    "\\end{bmatrix}$ then,\n",
    "\n",
    "$A^T = \\begin{bmatrix}\n",
    "    1 & 3 \\\\\n",
    "    2 & 4\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf501fbb-6471-4a6f-88f9-b030d60a3b85",
   "metadata": {},
   "source": [
    "**Properties**\n",
    "\n",
    "- $(AB)^T = B^TA^T$\n",
    "- $(A^T)^T = A$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94944c77-df18-4db3-84a6-099af380b80c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 8.4 Matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90280321-bc5b-4f85-b2c1-6ba30ef99ae5",
   "metadata": {
    "tags": []
   },
   "source": [
    "To multiply a matrix by another matrix we need to do the \"dot product\" of rows and columns, i.e. If \n",
    "\n",
    "$\n",
    "A = \\begin{bmatrix}\n",
    "    \\vec{a_1}^T \\\\\n",
    "    \\vec{a_2}^T \\\\\n",
    "    \\vdots \\\\\n",
    "    \\vec{a_n}^T\n",
    "\\end{bmatrix}\n",
    "$ and $\n",
    "B = \\begin{bmatrix}\n",
    "    \\vec{b_1} & \\vec{b_2} & \\dots & \\vec{b_k}\n",
    "\\end{bmatrix}\n",
    "$, then"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ef578b-53e7-438f-aad3-87c0c19745a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "$\n",
    "AB = \\begin{bmatrix}\n",
    "    A\\vec{b_1} & A\\vec{b_2} & \\dots & A\\vec{b_k}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    \\vec{a_1}^T\\vec{b_1} & \\vec{a_1}^T\\vec{b_2} & \\dots & \\vec{a_1}^T \\vec{b_k}\\\\\n",
    "    \\vec{a_2}^T\\vec{b_1} & \\vec{a_2}^T\\vec{b_2} & \\dots & \\vec{a_2}^T \\vec{b_k}\\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "    \\vec{a_n}^T\\vec{b_1} & \\vec{a_n}^T\\vec{b_2} & \\dots & \\vec{a_n}^T \\vec{b_k}\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a841f38b-fb4c-4696-8872-6398faf1a529",
   "metadata": {},
   "source": [
    "**special case**\n",
    "\n",
    "If B has only one column $\\vec{b}$, then\n",
    "\n",
    "$A\\vec{b}= \\begin{bmatrix}\n",
    "    \\vec{a_1}^T\\vec{b} \\\\\n",
    "    \\vec{a_2}^T\\vec{b} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\vec{a_n}^T\\vec{b}\n",
    "\\end{bmatrix} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44755be7-40a8-4c90-a64a-e995323b0586",
   "metadata": {},
   "source": [
    "If A has only one row $a^T$, then\n",
    "\n",
    "$\\vec{a}^TB = \\begin{bmatrix}\n",
    "    \\vec{a}^T\\vec{b_1} & \\vec{a}^T\\vec{b_2} & \\dots & \\vec{a}^T\\vec{b_k}\n",
    "\\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e0ff8e-ba03-4ad9-885f-dc82ffac7c51",
   "metadata": {},
   "source": [
    "> These two special cases show when you can factor out a vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a15ea-7e1b-4ff0-a087-eb4c6a7164eb",
   "metadata": {},
   "source": [
    "![matrix-multiplication.png](./images/matrix-multiplication.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db239c1-5c5d-4be5-b96f-b7bf5ed50fcd",
   "metadata": {},
   "source": [
    "With the definition of matrix multiplication, we have the following properties\n",
    "\n",
    " - $AB \\neq BA$\n",
    " - $A(B+C) = AB + AC$\n",
    " - $ABC = A(BC)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a3ded8-feca-448a-9b9e-9db708028b4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 8.5 Inverse of a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c264a981-2cc2-44a9-881e-312f703ef634",
   "metadata": {},
   "source": [
    "The Inverse of a matrix is a matrix which makes $A^{-1}A = I$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1342fa4-ee61-4683-a07f-169f736f04f8",
   "metadata": {},
   "source": [
    "Inverse matrix is used the mimic *division* of real numbers. As an example, If $AB = C$, we can rewrite the equation as $A = CB^{-1}$. (This is because $ABB^{-1} = CBB^{-1}$, $AI = C$, and therefore $A = C$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add8b000-1492-4fa1-a4bf-414ddf42d6c2",
   "metadata": {},
   "source": [
    "**properties**\n",
    "\n",
    "- $(AB)^{-1} = B^{-1}A^{-1}$\n",
    "- $(A^{-1})^{-1} = A$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f424f5e3-05ec-4b55-96b2-409251511e44",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 8.6 Matrix derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e4a110-c724-4b97-acb0-29f55f0dacf1",
   "metadata": {},
   "source": [
    "To mimic the derivative of a real valued function - for example, if y = $\\beta_0 + \\beta_1 x +u$, then $\\frac{\\partial y}{\\partial x}=\\beta_1$ - we define **vector (matrix with one column) derivatives** as "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbf191c-d98d-4191-a30c-9ccd184a804b",
   "metadata": {},
   "source": [
    "$\\frac{\\partial \\vec{y}_{n\\times 1}}{\\partial \\vec{x}_{k\\times_1}} = \\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\dots & \\frac{\\partial y_1}{\\partial x_k} \\\\\n",
    "\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\dots & \\frac{\\partial y_2}{\\partial x_k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_n}{\\partial x_1} & \\frac{\\partial y_n}{\\partial x_2} & \\dots & \\frac{\\partial y_n}{\\partial x_k}\n",
    "\\end{bmatrix}_{n\\times k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40ff5f6-9643-44b4-960e-d9f5f78bc10d",
   "metadata": {},
   "source": [
    "**properties**\n",
    "\n",
    "- if $y = Ax$ then $\\frac{\\partial \\vec{y}}{\\partial \\vec{x}} = A$\n",
    "- if $z = Ay$ and $y = Bx$, then $\\frac{d \\vec{z}}{d \\vec{x}} = \\frac{\\partial \\vec{z}}{\\partial \\vec{y}}\\frac{\\partial \\vec{y}}{\\partial \\vec{x}} = AB$ - chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c8926b-490b-4afb-867b-925cef04e984",
   "metadata": {},
   "source": [
    "- if a scalar $ c = \\vec{y}^TA\\vec{x}$, then $\\frac{\\partial c}{\\partial \\vec{x}} = \\vec{y}^TA$ and $\\frac{\\partial c}{\\partial \\vec{y}} = (A\\vec{x})^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbb7653-acf7-4d9c-a877-285529402850",
   "metadata": {},
   "source": [
    "- if a scalar $c = \\vec{y}^T A \\vec{x}$, then $\\frac{d c}{d \\vec{z}} = \\frac{\\partial c}{\\partial \\vec{x}} \\frac{\\partial \\vec{x}}{\\partial \\vec{z}} + \\frac{\\partial c}{\\partial \\vec{y}} \\frac{\\partial \\vec{y}}{\\partial \\vec{z}} = \\vec{y}^TA \\frac{\\partial \\vec{x}}{\\partial \\vec{z}} + \\vec{x}^TA^T \\frac{\\partial \\vec{y}}{\\partial \\vec{z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cccf99e-955b-4674-bdaa-598cba0939c7",
   "metadata": {},
   "source": [
    "- if a scalar $ c = \\vec{x}^TA\\vec{x}$, then $\\frac{\\partial c}{\\partial \\vec{x}} = \\vec{x}^T(A+A^T)$\n",
    "- if a scalar $ c = \\vec{x}^TA\\vec{x}$ and A is symmetric, then $\\frac{\\partial c}{\\partial \\vec{x}} = 2\\vec{x}^TA$\n",
    "- if a scalar $ c = \\vec{x}^T\\vec{x}$, then $\\frac{\\partial c}{\\partial \\vec{x}} = 2\\vec{x}^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe2b375-75a3-49e5-9976-85d452a88b6a",
   "metadata": {},
   "source": [
    "# 8.7 Convert linear equation to matrix form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095fcc9-2695-429a-93aa-52a976fa7ec5",
   "metadata": {},
   "source": [
    "We've learned so many matrix operations. The next the question is how can we convert a linear equation to its matrix form so that we can utilize all of the properties mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63279638-e7d5-426f-b8f2-67be2b478cda",
   "metadata": {},
   "source": [
    "> Indeed, the properties of matrices are derived the other way round. We first convert matrix operations to their linear equation forms and then prove the properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2b44bf-05af-4dae-9cf1-9636d7327240",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summation\n",
    "\n",
    "$\\sum_k x_k\\beta_k$ can be expressed as $\\vec{x}^T \\vec{\\beta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b00b6c9-abde-436c-8bc4-afc90ff57517",
   "metadata": {},
   "source": [
    "**special case**\n",
    "$\\sum_k x_k^2 = \\vec{x}^T \\vec{x}$ this is also denoted as $||\\vec{x}||_2^2$ - $L_2$ norm of a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e765df4f-27ae-4c7b-afda-d8cea83adae7",
   "metadata": {},
   "source": [
    "### Double summation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e606196c-81d1-48d5-85f8-6ad8572e14bd",
   "metadata": {},
   "source": [
    "$\\sum_i\\sum_j a_{ij} x_i y_j$ can be rewritten as $\\vec{x}^TA\\vec{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f6a793-b95a-4867-9cea-81cef3f9b5fe",
   "metadata": {},
   "source": [
    "**special case 1**: $\\vec{x}^TA\\vec{x}$ is called the *quadratic form* of A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382732f4-99cd-4c83-bcf4-2e15d346c353",
   "metadata": {},
   "source": [
    "**special case 2**: $(\\sum_i x_i)^2 = \\vec{x}^T \\textbf{1}_{n\\times n} \\vec{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8248d66-d404-45dc-a562-b9cd54db61ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Stacking - equation system\n",
    "\n",
    "If for each row of the equation system i, we have $\\sum_k x_{ik}\\beta_k = y_i$. Then by stacking the equations we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ff0d35-dd1e-4009-ab29-4b23171546d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "$\n",
    "\\begin{bmatrix}\n",
    "    \\sum_k x_{1k}\\beta_k \\\\\n",
    "    \\sum_k x_{2k}\\beta_k \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sum_k x_{nk}\\beta_k \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "    y_1 \\\\\n",
    "    y_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    y_n\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec75d248-f582-4a4d-b520-ea38f7bc45d3",
   "metadata": {},
   "source": [
    "Then we can rewrite each row to its matrix form and stack them together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89186910-5182-416e-b4ce-4dec361fff13",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{bmatrix}\n",
    "    \\vec{x_1}^T\\vec{\\beta} \\\\\n",
    "    \\vec{x_2}^T\\vec{\\beta} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\vec{x_3}^T\\vec{\\beta} \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "    y_1 \\\\\n",
    "    y_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    y_n\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b015ca-e7b9-432b-9ca9-ea5623067318",
   "metadata": {},
   "source": [
    "b can be factored out using matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa72fe4f-3939-4462-aa9e-a27e15848da0",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{bmatrix}\n",
    "    \\vec{x_1}^T \\\\\n",
    "    \\vec{x_2}^T \\\\\n",
    "    \\vdots \\\\\n",
    "    \\vec{x_3}^T \n",
    "\\end{bmatrix}\\vec{\\beta} = \n",
    "\\begin{bmatrix}\n",
    "    y_1 \\\\\n",
    "    y_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    y_n\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e989ef3-51f4-40fe-8ed8-4661e91c04dd",
   "metadata": {},
   "source": [
    "This is often rewritten as $X\\vec{\\beta} = \\vec{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379f5153-5236-418c-9887-f94bf4efe6b5",
   "metadata": {},
   "source": [
    "# 8.8 Application - OLS\n",
    "\n",
    "Consider a model $y=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + u$, then in matrix form this becomes\n",
    "\n",
    "$$y = \\vec{x}^T\\vec{\\beta}+ u$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc03fba-6a51-4198-8138-32814019d84d",
   "metadata": {},
   "source": [
    "This equation holds for each observation, therefore we can stack them together and get\n",
    "$$\\vec{y} = X\\vec{b}+\\vec{u}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb3487a-41cd-4f80-9a36-012650105371",
   "metadata": {},
   "source": [
    "Apply the OLS method where we solve \n",
    "\n",
    "$$\\min_{\\beta_0,\\beta_1,\\beta_2} \\sum_i [y_i-(\\beta_0+\\beta_1 x_{i1} + \\beta_2 x_{i2})]^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f47543-b846-4588-8b86-cfe77101676f",
   "metadata": {},
   "source": [
    "This is equivalent to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666416f7-ae2c-48e6-9bde-c726b80ab1d3",
   "metadata": {},
   "source": [
    "$$\\min_{\\beta_0,\\beta_1,\\beta_2} \\sum_i u_i^2 = \\vec{u}^T\\vec{u}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0278935c-9730-49d7-b2ea-5a4d514356b6",
   "metadata": {},
   "source": [
    "Since $\\vec{u} = \\vec{y} - X\\beta$, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c275c207-3c26-4126-9a20-92019661ef00",
   "metadata": {},
   "source": [
    "$$\\min_\\vec{\\beta} (\\vec{y} - X\\vec{\\beta})^T(\\vec{y}-X\\vec{\\beta})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89c5da1-a1c5-4eac-bdad-b34ebb84c1b5",
   "metadata": {},
   "source": [
    "To solve this optimization problem, we find the first order condition, let $v = (\\vec{y} - X\\vec{\\beta})^T(\\vec{y}-X\\vec{\\beta})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042f092a-8283-40cf-9d3e-01964bbfda10",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial v}{\\partial \\vec{\\beta}} = \\vec{0}_{1\\times 3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde1a476-bfcb-4e61-8b2e-63f87719f499",
   "metadata": {},
   "source": [
    "Apply matrix derivatives chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0d5135-290c-429f-9146-a90a63220497",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial v}{\\partial \\vec{\\beta}} = 2(\\vec{y}-X\\vec{\\beta})^T\\frac{\\partial (\\vec{y}-X\\vec{\\beta})}{\\partial \\vec{\\beta}} = 2(\\vec{y}-X\\vec{\\beta})^TX = \\vec{0}_{1\\times 3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639524d0-c163-41e1-be95-ce488e51ab75",
   "metadata": {},
   "source": [
    "Transpose both sides of the last equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5932e369-4024-48fd-b892-9d899c235638",
   "metadata": {},
   "source": [
    "$$2X^T(\\vec{y}-X\\vec{\\beta})=\\vec{0}_{3\\times 1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be220e1-eb7d-4390-8eca-dd206700b380",
   "metadata": {},
   "source": [
    "Rearrange the equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b188a85-a0db-4a43-b7e9-54b8edceba93",
   "metadata": {},
   "source": [
    "$$X^TX\\vec{\\beta} = X^T\\vec{y}$$\n",
    "$$\\vec{\\hat{\\beta}} = (X^TX)^{-1}X^T\\vec{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8193e398-0041-4e99-9f13-cc28c2d26675",
   "metadata": {},
   "source": [
    "# 8.9 Application - Ridge Regression\n",
    "\n",
    "As mentioned in chapter 7-2. Ridge regression tries to minimize MSE by deliberately introduce bias - in order to reduce variance.\n",
    "\n",
    "The motivation of ridge regression is direct. If more variables are introduced into the model, there will be more $\\beta$s waiting to be estimated. To limit the number of variables, we can just add a penalty to the minimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b9d75f-f5d7-4aa8-8fd0-82f8bf457315",
   "metadata": {},
   "source": [
    "$$\\min_{\\vec{\\beta}} ||\\vec{y}-X\\vec{\\beta}||_2^2 + \\alpha||\\vec{\\beta}||_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33597c60-4ec2-4a23-adc5-f2b1238c6a66",
   "metadata": {},
   "source": [
    "where $||\\cdot||_2^2$ is the square of $L_2$ norm, and $\\alpha$ is called a meta parameter, which means it should be preset by us and then solve the minimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b14e20c-b3fc-4926-acfb-13089827cefe",
   "metadata": {},
   "source": [
    "Taking derivatives and solve the first order condition, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede360f2-c2db-473a-9893-a4f8ba1a2805",
   "metadata": {},
   "source": [
    "$$\\vec{\\hat{\\beta}} = (X^TX+\\alpha I)^{-1}X^T\\vec{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e72a40-f26f-452e-af57-e74f86a337de",
   "metadata": {},
   "source": [
    "Look, we added a \"ridge\" (diagonal matrix) to $X^TX$, hence the name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b16af7d-804f-4af7-bb14-aecffc4f2088",
   "metadata": {},
   "source": [
    "# 8.10 Application - Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d2bb0c-ce75-494d-8a27-1fc049d9a889",
   "metadata": {},
   "source": [
    "Lasso stands for *Least Absolute Shrinkage and Selection Operator*, and it is similar to the ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944f0a6f-cba6-40c3-b855-52b8db532662",
   "metadata": {},
   "source": [
    "$$\\min_{\\vec{\\beta}} \\frac{1}{2n}||\\vec{y}-X\\vec{\\beta}||_2^2 + \\alpha||\\vec{\\beta}||_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dfbc3d-5db4-443f-b052-7bf3cf706736",
   "metadata": {},
   "source": [
    "Instead of using $L_2$ norm, Lasso uses the $L_1$ norm of vector $\\vec{\\beta}$. \n",
    "\n",
    "> $||\\beta||_1 = \\sum_p |\\beta_p|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd79a06-4940-4ec6-b63d-c34fc5967f2b",
   "metadata": {},
   "source": [
    "Hence, Lasso is also known as the $L_1$ regularization, and Ridge as the $L_2$ regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c25d0-fbbb-49d5-909f-47747b9cf905",
   "metadata": {},
   "source": [
    "# 8.11 Python matrix operation - numpy & patsy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef921abc-3dff-42ef-85df-b106f5b4edf2",
   "metadata": {},
   "source": [
    "**Numpy** supports matrix operations and **patsy** is used for building design matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c22b7c-a7c6-4d87-aa69-1bea011c7d0f",
   "metadata": {},
   "source": [
    "Consider a model $colGPA = \\beta_0 + \\beta_1 hsGPA +\\beta_2 ACT + u$. Instead of using statsmodels, we can calculate the estimators using matrix operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0570d8f3-a3c8-4fbd-b197-5ef8e631c2ed",
   "metadata": {},
   "source": [
    "First, we need the design matrix X, whose first column contains n ones, and the other columns contains the the observations of independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6844ecd-065f-49f7-a114-0182f87dde50",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.       ]\n",
      " [3.4000001]\n",
      " [3.       ]\n",
      " [3.5      ]\n",
      " [3.5999999]]\n",
      "[[ 1.          3.         21.        ]\n",
      " [ 1.          3.20000005 24.        ]\n",
      " [ 1.          3.5999999  26.        ]\n",
      " [ 1.          3.5        27.        ]\n",
      " [ 1.          3.9000001  28.        ]]\n"
     ]
    }
   ],
   "source": [
    "import wooldridge as woo\n",
    "import patsy as pt\n",
    "\n",
    "df = woo.data(\"gpa1\")\n",
    "\n",
    "#create the design matrix (and the y vector) using patsy\n",
    "y, X = pt.dmatrices(\"colGPA~hsGPA+ACT\",data=df)\n",
    "# pt.dmatrix(\"hsGPA+ACT\", data=df) function will return X only\n",
    "print(y[:5]) \n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c6db95-6dc0-43d2-9ff5-580c793e13cc",
   "metadata": {},
   "source": [
    "Next, we can calculate $\\hat{\\beta}$ using the formula: $\\vec{\\hat{\\beta}} = (X^TX)^{-1}X^T \\vec{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d4dee4-5bb3-4b06-9623-f1bb2d2a699a",
   "metadata": {},
   "source": [
    "This equation involves three matrix operations\n",
    "- Transpose: X.T in numpy\n",
    "- Matrix multiplication: X.T @ X in numpy\n",
    "- Inverse: np.linalg.inv(X.T @ X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18b5bcc3-6b21-4cfd-80bf-5fe3988449de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5e998f3-b725-4cbd-9453-71803ef1428b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.28632777]\n",
      " [0.45345589]\n",
      " [0.00942601]]\n"
     ]
    }
   ],
   "source": [
    "b_h = np.linalg.inv(X.T@X)@X.T@y\n",
    "print(b_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0012dd2-1d5d-4c4e-83be-3b0712c82a6e",
   "metadata": {},
   "source": [
    "Other Examples:\n",
    "- Elementwise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e39fd42e-e56a-4718-86bd-8a118061fd62",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  6],\n",
       "       [12, 20]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,2],[3,4]])\n",
    "B = np.array([[2,3],[4,5]])\n",
    "A*B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3046d2b3-440d-4442-b0c5-fcc6bdb51936",
   "metadata": {},
   "source": [
    "- Eigen value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b062672d-c743-4944-bd87-b253cb7ed480",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.37228132,  5.37228132]),\n",
       " array([[-0.82456484, -0.41597356],\n",
       "        [ 0.56576746, -0.90937671]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be505f-dd08-4a3a-af94-66efbe4935bb",
   "metadata": {},
   "source": [
    "- Determinant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c2546c2-bd8f-451f-b1af-7c00d637e406",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.0000000000000004"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2d9435-8fd3-40eb-a5e6-7177f1c84024",
   "metadata": {},
   "source": [
    "- Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8e9099f-c591-4807-8055-b91acc27adfa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.matrix_rank(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b9a18d-217c-40bd-adc7-ca1a828ca21f",
   "metadata": {},
   "source": [
    "### Exercise,\n",
    "\n",
    "The estimated variance of u is given by $s^2 = \\frac{1}{n-k-1}\\sum \\hat{u_i}^2=\\frac{1}{n-k-1}\\vec{\\hat{u}}^T\\vec{\\hat{u}}$, and the variance-covariance matrix of $\\vec{\\hat{\\beta}}$ is given by $s^2(X^TX)^{-1}$. Try to calculate the variance-covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a814c344-b700-44ac-ad7a-a72287bc2119",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.16159717e-01, -2.26063687e-02, -1.59084858e-03],\n",
       "       [-2.26063687e-02,  9.18011489e-03, -3.57076664e-04],\n",
       "       [-1.59084858e-03, -3.57076664e-04,  1.16147776e-04]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_h = y-X@b_h\n",
    "s_2 = u_h.T@u_h/(len(u_h)-X.shape[1])\n",
    "vcov = s_2*np.linalg.inv(X.T@X)\n",
    "vcov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceb08372-a6b3-46f3-a100-51a51b5bea26",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34082212, 0.09581292, 0.01077719])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the diagonal values\n",
    "var = np.diagonal(vcov)\n",
    "\n",
    "# Take the square root of variances to get the standard error\n",
    "np.sqrt(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "356853d4-e748-4062-af90-e49e9283b7af",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept    0.340822\n",
       "hsGPA        0.095813\n",
       "ACT          0.010777\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare the result with smf\n",
    "import statsmodels.formula.api as smf\n",
    "smf.ols(\"colGPA~hsGPA + ACT\",data=df).fit().bse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
