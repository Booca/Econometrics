{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d6f071-1daf-48a3-85fe-c02baaedfee5",
   "metadata": {},
   "source": [
    "# 7-2 Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a35539-28b7-4c6d-a04b-e774e968241c",
   "metadata": {},
   "source": [
    "The biasâ€“variance tradeoff is frequently mentioned in statistics, econometrics, and machine learning. Though bearing the same name, the tradeoff between bias and variance of an estimator displays different mathematical implication in different contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f0cdc9-c088-4e82-8e31-5dc993b31c2f",
   "metadata": {},
   "source": [
    "## 7.1 Bias-Variance Tradeoff in Econometrics\n",
    "\n",
    "### Omitted Variable Bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928f7369-b1dd-4452-8a06-d4d5e30642e5",
   "metadata": {},
   "source": [
    "Consider the true data generating process is:\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 +u$$\n",
    "\n",
    "with $u \\sim N(0,\\sigma^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3fbb41-6174-463b-8083-4f58c2cd21a7",
   "metadata": {},
   "source": [
    "Our question is, what if we do not have variable $x_2$ in the data. In order word, what if we specified a wrong model - $y = \\beta_0 + \\beta_1 x_1 + u$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc7c6fd-e4cd-4ff5-8c8d-a700316f1740",
   "metadata": {},
   "source": [
    "In that case, we will run an OLS with an erroneous specification.\n",
    "\n",
    "$$min_{\\beta_0,\\beta_1} \\sum_i (y_i - \\beta_0 - \\beta_1 x_1)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b3c20-0cb3-4d05-976e-b5248cdfbc1b",
   "metadata": {},
   "source": [
    "The solution to which is:\n",
    "\n",
    "$$\\hat{\\beta_0} = \\bar{y}-\\hat{\\beta_1}\\bar(x_1)$$\n",
    "$$\\hat{\\beta_1} = \\frac{\\sum(y_i-\\bar{y})(x_{1i}-\\bar{x_1})}{\\sum(x_{1i}-\\bar{x_1})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70bf3c6-449b-47eb-8c6e-6789abbd7eaf",
   "metadata": {},
   "source": [
    "### Unbias\n",
    "\n",
    "One important goal of statistical inference is to obtain an unbiased point estimator - i.e. when the sample changes (the unobservables change), the expected value of estimator is equal to the population parameter. In mathematical terms, this means\n",
    "\n",
    "$$E_u[\\hat{\\beta}] = \\beta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b90d860-15a7-45c2-a2d4-ab962bd65933",
   "metadata": {},
   "source": [
    "You can also treat $x_1 and x_2$ as random variables, but that will make the calculation more difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea51f1e6-5af8-4abc-82f8-f1afe7fae357",
   "metadata": {},
   "source": [
    "Assume we are interested in estimating the slope parameter $\\beta_1$. Let's see if $\\hat{\\beta_1}$ is an unbiased estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863111c9-6931-463d-80ad-ed895e4a54af",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    E_u[\\hat{\\beta_1}] &= \\frac{1}{\\sum (x_{1i}-\\bar{x_1})^2}E_u[\\sum y_i(x_{1i}-\\bar{x_1})] - 0 \\\\\n",
    "    &=\\frac{1}{\\sum (x_{1i}-\\bar{x_1})^2}E_u[\\sum (\\beta_0 + \\beta_1 x_{1i} + \\beta-2 x_{2i} + u)(x_{1i}-\\bar{x_1})] \\\\\n",
    "    &=\\frac{1}{\\sum (x_{1i}-\\bar{x_1})^2}E_u[\\sum (\\beta_1 x_{1i} + \\beta_2 x_{2i})(x_{1i}-\\bar{x_1})] \\\\\n",
    "    &=\\frac{\\beta_1 E_u[\\sum x_{1i}(x_{1i}-\\bar{x_1})]}{\\sum (x_i-\\bar{x})^2} + \\frac{E_u[\\sum \\beta_2 x_{2i}(x_{1i}-\\bar{x_1})]}{\\sum (x_{1i}-\\bar{x_1})^2} \\\\\n",
    "    &=\\beta_1+\\frac{\\beta_2  \\sum x_{2i}(x_{1i}-\\bar{x_1})}{\\sum (x_{1i}-\\bar{x_1})^2}                      \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3457322d-0a88-4bb0-85c8-3ebe3d09d96b",
   "metadata": {},
   "source": [
    "$\\hat{\\beta_1}$ is not an unbiased estimator if the sample covariance of $x_2$ and $x_1$ is not equal to 0, and $\\beta_2$ is not equal to zero. (If you treat $x_1$ and $x_2$ as random variables, then it means $Cov(x_1,x_2)\\neq 0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44a5f3a-1057-4d38-810f-2fc6f36b463d",
   "metadata": {},
   "source": [
    "Hence, we can draw a conclusion that if an relevant variable is omitted from the model specification, and the omitted variable is correlated with the variable of interest, OLS estimators will be biased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7078a5-46cc-424c-8d3d-c851a4fff8dd",
   "metadata": {},
   "source": [
    "### Inflated Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4eb2da-5112-4260-b485-45b94678e713",
   "metadata": {},
   "source": [
    "One apparent solution to the omitted bias problem is to collect data on $x_2$ and include it in the model specification. This will make $\\hat{\\beta_1}$ unbiased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024e6d4c-7879-4dc8-bbfc-fc5fd0c0b4c1",
   "metadata": {},
   "source": [
    "But this remedy is based on the assumption that you already know the true data generating process, which is unlikely in real life - In real-life application, our starting point is a dataset, we can only \"guess\" how GOD designs the mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa3d48f-da8c-4d66-bfc9-e845851e22ee",
   "metadata": {},
   "source": [
    "Alternatively, we can always add as many variables as possible into the specification. This method will act as a safety net and can be shown to yield unbiased estimators as long as the residuals satisfies Gauss-Markov Assumption 1: E[u|x]=0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7253d0-93d6-4e32-9d1b-5a3a7d14c7fb",
   "metadata": {},
   "source": [
    "However, there is a **disadvantage** with this alternative method - adding more variables will inflate the variance of the estimator. To see this, let's consider the omitted variable example we used in the previous section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eee7b1-fa5c-4ccf-b28c-17fda657544e",
   "metadata": {},
   "source": [
    "Using the misspecified model $y=\\beta_0 + \\beta_1 x_1 + u$, the variance of the estimator $\\hat{\\beta_1}$ is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a4b8b-dd7f-4f5a-a80a-ba04ead5fc63",
   "metadata": {},
   "source": [
    "$$Var_u(\\hat{\\beta_1}) = \\frac{\\sigma^2}{n Var(x_1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d94bcf0-632b-4ddf-94ac-3ec215f119fe",
   "metadata": {},
   "source": [
    "After adding $x_2$ into the model and rerun OLS, the variance of $\\hat{\\beta_1}^{more}$ becomes\n",
    "$$Var_u(\\hat{\\beta_1}^{more}) = \\frac{\\sigma^2}{n Var(x_1)}\\frac{1}{1-R_1^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6361b5be-7b2a-4171-bba3-af46066f15fe",
   "metadata": {},
   "source": [
    "where $R_1^2$ is the r-squared from regression $x_1$ on $x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decd81c2-0c10-446d-aa37-324f79c69906",
   "metadata": {},
   "source": [
    "If $x_1$ and $x_2$ are not perfectly uncorrelated, $R_1^2>0$, and $\\frac{1}{1-R_1^2}$ will be greater than 1, therefore \"inflate\" the first term in variance. This leads to $Var(\\hat{\\beta_1}^{more})>Var(\\hat{\\beta_1}^{less})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c21c4f-76f0-4622-8161-095f3b6c734d",
   "metadata": {},
   "source": [
    "> **Conclusion** \\\n",
    "Adding nonorthogonal (i.e. $Cov(x_{add},x_{rest})\\neq 0$) variables into the model will increase the variance the of the old estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378c4903-ec94-4763-8d8d-e5db9bcb4fa0",
   "metadata": {},
   "source": [
    "## 7.2 Bias-Variance Tradeoff in Machine learning\n",
    "\n",
    "Machine learning focus on the predicted value instead of individual coefficients. In this regard, let's focus on the bias and variance of the predicted value $\\hat{y}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94283b8a-abef-4b6b-b75f-1a2d45110317",
   "metadata": {},
   "source": [
    "Since the goal of machine learning is to make the \"best\" predictions, we need to first set up a numerical \"measure\" to evaluate how \"good\" a prediction is. Such a measure is known as the **loss (or goal) function**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cc2e41-eebd-412c-a4f9-0e2ed48ff4fb",
   "metadata": {},
   "source": [
    "A loss function closely related to OLS is the mean squared error loss.\n",
    "\n",
    "$$MSE = E[(y - \\hat{y})^2]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd68e71-46b1-4cff-a15e-704229faa6a3",
   "metadata": {},
   "source": [
    "To obtain $\\hat{y}$, the statistical convention is to estimate a sample version of the loss function, i.e.\n",
    "\n",
    "$$\\min_{\\beta_0,\\beta_1,...\\beta_k} \\frac{1}{N}\\sum_i (y_i - \\hat{y})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482f4654-076a-4de7-99f1-958e7661d3c5",
   "metadata": {},
   "source": [
    "> Intuition: to minimize the loss function in the population, we should first minimize a similar loss function using the sample (the dataset we have)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c18ab-184d-449f-9d55-42cdd1e6ca7d",
   "metadata": {},
   "source": [
    "Note that this minimization problem is exactly the same as an OLS regression. Hence the estimators and predictions will also be the same as OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec43679-42f1-44c2-9a4e-cb56b96fca3a",
   "metadata": {},
   "source": [
    "### Bias-Variance Decomposition of MSE\n",
    "\n",
    "People soon became unsatisfied with the OLS type of estimation, and started seeking other ways to further minimize the population loss function - MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7450e8ee-9d4f-4dd6-b59f-f9b812e6b954",
   "metadata": {},
   "source": [
    "> NOTE: Econometricians only care about the coefficients, therefore there is no need to pursue a better prediction (smaller loss) given that an unbiased and efficient estimator is already generated by OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2eeccd-cb08-46a9-88d7-662cb0873071",
   "metadata": {},
   "source": [
    "To achieve this goal, let's first gain a better understanding of the components of MSE. A decomposition can help us target the minimization effort at a specific part of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2fd9be-7f0f-4482-9eec-4f08f335d9d7",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    MSE &\\equiv E_u[(y - \\hat{y})^2]\\\\\n",
    "        &= E[(f+u-\\hat{y})^2] \\\\\n",
    "        &= E[(f+u-\\hat{y}+E[\\hat{y}]-E[\\hat{y}])^2] \\\\\n",
    "        &= E[[(f-E[\\hat{y}]) - (\\hat{y}-E[\\hat{y}])+u]^2] \\\\\n",
    "        &= E[(f-E[\\hat{y}])^2 + (\\hat{y}-E[\\hat{y}])^2 + u^2  - 2(f-E[\\hat{y}])(\\hat{y}-E[\\hat{y}])) + 2(f-E[\\hat{y}])u - 2 (\\hat{y}-E[\\hat{y}])u]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43a7df1-eb67-4097-9ab7-4b27d9cfb237",
   "metadata": {},
   "source": [
    "By assumption $E[u] = 0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f066ff-32e4-41fa-9244-a569b2e0f912",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "MSE &= E[(f-E(\\hat{y}))^2] + E[(\\hat{y}-E[\\hat{y}])^2] + \\sigma^2 \\\\\n",
    "& = (f-E[\\hat{y}])^2 + Var(\\hat{y}) + \\sigma^2 \\\\\n",
    "&\\equiv Bias(\\hat{y})^2 + Var(\\hat{y}) + \\sigma^2\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306aab8a-62b3-4464-a9d1-daa246367385",
   "metadata": {},
   "source": [
    "> In statistics, the difference between the true parameter $\\beta$ and the expected value of an estimator $E[\\hat{\\beta}]$ is called the bias of the estimator $\\hat{\\beta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c3b3b4-5486-4ad0-8368-0bc8f09f400f",
   "metadata": {},
   "source": [
    "### Bias-Variance Tradeoff\n",
    "The Bias-Variance decomposition of the MSE loss shows us that we can lower MSE by either reducing the bias of the prediction or by reducing the variance of the prediction - This is a trivial conclusion, one that can be drawn w/o math."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d775526-235e-442c-b849-4cdd3169bfc6",
   "metadata": {},
   "source": [
    "<img src=\"./images/bias-variance3.png\" style=\"width: 50%; margin: auto\" alt=\"\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf6481-57cd-4467-8bba-005a74522e7f",
   "metadata": {},
   "source": [
    "The question is rather: how can we reduce $Bias(\\hat{y})$ or $Var(\\hat{y})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1ae8ef-264c-4b91-b96a-460342673cc0",
   "metadata": {},
   "source": [
    "### Solution 1 - add in new variables or high ordered terms\n",
    "This idea comes from section 7.1, the omitted variable bias. By introducing possibly omitted variables, we can reduce the bias of the estimated coefficient, which leads to a reduced bias of prediction. To see this, we have\n",
    "\n",
    "$$Bias(\\hat{y}^{less}) = Bias(\\hat{\\beta_0}+\\hat{\\beta_1}x_1) \\neq 0,$$\n",
    "\n",
    "in the presence of an omitted variable $x_2$. After adding $x_2$ to the model, \n",
    "$$Bias(\\hat{y}^{more}) = Bias(\\hat{\\beta_0}^{more}+\\hat{\\beta_1}^{more}x_1 + \\hat{\\beta_2}^{more}x_2) = 0 $$ \n",
    "because when using the correct specification, OLS yields unbiased estimators.\n",
    "\n",
    "But at the same time, $Var(\\hat{y})$ will be higher due to two reasons: (1) $Var(\\hat{y}^{more})$ has one more term inside the parenthesis than $Var(\\hat{y}^{less})$; (2) Variance of the old parameters are higher if $x_2$ is non-orthogonal to the other variables.\n",
    "\n",
    "Hence we have a tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abef6221-43e3-44cd-8e9d-9d9b2683cefc",
   "metadata": {},
   "source": [
    "**Graphical display of the Tradeoff** - assume dgp is known"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e05d735-32da-40fb-8d3d-09011d5f0d0d",
   "metadata": {},
   "source": [
    "![bias-variance.png](./images/bias-variance1.png)\n",
    "\n",
    "The image above is a classical representation of the bias-variance tradeoff as higher ordered polynomials are added into the model. The left panel fit models with different polynomial flexibility to the same dataset:\n",
    "- Yellow line: $y = \\beta_0 + \\beta_1 x + u$\n",
    "- Black curve: $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + u$\n",
    "- Blue curve: $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + ...+ \\beta_{10} x^{10} + u$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c36839e-281a-4815-a312-1f4ff396d121",
   "metadata": {},
   "source": [
    "> Note that high-order polynomials are rarely used in econometrics because the starting point of econometrics should be a known economic theory - to prevent inflated variance, while also guarantee an unbiased estimator as mentioned in section 7.1. And in economic theories, high-order polynomials are rare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9f414d-5079-46e3-9344-de916dc5718b",
   "metadata": {},
   "source": [
    "The right panel displays the value of MSE  (assume the data generating process is known) as more polynomial terms are added into the model.\n",
    "\n",
    "Switching from a linear function to a cubic function significantly reduces MSE. This is because the true model contains terms higher than the $x^3$. The reduction in Bias is higher than the increase in Variance.\n",
    "\n",
    "Switching from a cubic function to a 10th-ordered function still reduces MSE. But if more terms are added into the mode (green dot in the right panel), the increase in variance will dominate the aggregate effect and drives MSE up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa147fc-a24e-48a5-8d2e-5088fe96655b",
   "metadata": {},
   "source": [
    "**Graphical display of Tradeoff** - using train and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d7ad5b-daf0-4627-9266-601ec18c3748",
   "metadata": {},
   "source": [
    "Although the previous figure is very illustrative, it is not possible to plot when we do not know the d.g.p.\n",
    "\n",
    "Alternatively, we can plot the sample MSE using a test dataset to mimic the situation in a population. This involves a train/test splitting at the beginning of the analysis. Models will be only fitted to the train data, and the test data is left-out for model evaluation.\n",
    "\n",
    "![bias-var](./images/bias-variance2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a267bf30-9275-4d27-8369-6170418cd4e5",
   "metadata": {},
   "source": [
    "In the right panel, the y-axis shows the *sample* mean squared error - $\\frac{1}{n}\\sum_i (y_i-\\hat{y})^2$. The red line displays the sample MSE using the test data as more polynomial terms are added into the model. The curve is U-shaped, and the lowest sample MSE is obtained when the \"blue\" model (cubic function) is used.\n",
    "\n",
    "The gray line has nothing to do with the bias-variance tradeoff. It shows the sample MSE using train data. It decreases in the number of variables simply because you are enlarging the choice set.\n",
    "\n",
    "> Originally you are choosing $\\beta_0, \\beta_1$ to minimize sum of squared error, after adding in one more variable, you can choose over all possible values of $\\beta_0, \\beta_1, and \\beta_2$. By choosing $\\hat{\\beta_2}=0$, you can obtain at least the same SSE as the less-termed model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4458606-77eb-48d5-b6c7-59d16ec1204e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Solution 2: use other regression methods\n",
    "\n",
    "Another solution to reduce MSE is by deliberately introducing bias with the hope that the variance will reduce at a higher rate. You can refer to the Ridge regression and the LASSO model. Regularization methods as such introduce bias into the regression solution that can reduce variance considerably relative to the ordinary least squares (OLS) solution. Although the OLS solution provides non-biased regression estimates, the lower variance solutions produced by regularization techniques provide superior MSE performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
